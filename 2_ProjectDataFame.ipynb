{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90765670",
   "metadata": {},
   "source": [
    "**Imports and Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055cd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       # data manipulation\n",
    "import numpy as np        # numerical operations\n",
    "import re                 # regular expressions\n",
    "from pathlib import Path  # handle filesystem paths\n",
    "\n",
    "# function to load excel file and print rows and columns\n",
    "def load_excel(path: Path, filename: str):\n",
    "    file = path / filename                          # create full path\n",
    "    if not file.exists():                           # check if file exists\n",
    "        raise FileNotFoundError(f\"File not found: {file}\")\n",
    "    df = pd.read_excel(file, engine=\"openpyxl\")     # read excel file with openpyxl\n",
    "    print(f\"Loaded {filename}: {df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "    return df\n",
    "\n",
    "# function to normalize text columns with a list of steps\n",
    "def normalize_columns(df: pd.DataFrame, transformations: dict):\n",
    "    for col, steps in transformations.items():        # iterate over column and steps\n",
    "        if col not in df.columns:                     # skip if column not present\n",
    "            continue\n",
    "        s = df[col].astype(\"string\")                  # convert column to string dtype\n",
    "        for step in steps:                            # iterate transformations\n",
    "            if isinstance(step, str):\n",
    "                if step == \"strip\":\n",
    "                    s = s.str.strip()                # remove leading/trailing spaces\n",
    "                elif step == \"lower\":\n",
    "                    s = s.str.lower()                # convert to lowercase\n",
    "                elif step == \"upper\":\n",
    "                    s = s.str.upper()                # convert to uppercase\n",
    "                elif step == \"title\":\n",
    "                    s = s.str.title()                # title case\n",
    "                else:\n",
    "                    if hasattr(s.str, step):        # fallback for other string methods\n",
    "                        s = getattr(s.str, step)()\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown step '{step}' for column {col}\")\n",
    "            elif isinstance(step, (tuple, list)) and step[0] == \"replace\":\n",
    "                _, pat, repl, *rest = step\n",
    "                regex = rest[0] if rest else False\n",
    "                s = s.str.replace(pat, repl, regex=regex)   # replace pattern\n",
    "            elif callable(step):\n",
    "                try:\n",
    "                    res = step(s)                     # try vectorized\n",
    "                    if isinstance(res, (pd.Series, np.ndarray, list)):\n",
    "                        s = pd.Series(res, index=s.index)\n",
    "                    else:\n",
    "                        s = s.apply(step)\n",
    "                except Exception:\n",
    "                    s = s.apply(step)                 # fallback to apply\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported transformation step: \" + repr(step))\n",
    "        df[col] = s                                  # assign back to df\n",
    "    return df\n",
    "\n",
    "# split comma-separated strings into list and strip spaces\n",
    "def split_and_strip(s):\n",
    "    if pd.isna(s) or str(s).strip() == '':\n",
    "        return []\n",
    "    return [item.strip() for item in str(s).split(',') if item.strip() != '']\n",
    "\n",
    "# normalize urls (lowercase, strip, remove query)\n",
    "def make_url_clean(series: pd.Series):\n",
    "    s = series.fillna('').astype(str).str.strip().str.lower() #normalize text\n",
    "    s = s.str.replace(r'\\?.*$', '', regex=True)  # remove query params\n",
    "    s = s.replace({'': np.nan})                  # empty -> NaN\n",
    "    return s\n",
    "\n",
    "# extract slug from url (title of the movie in the url)\n",
    "def extract_title_from_url(series: pd.Series) -> pd.Series:\n",
    "    s = series.fillna('').astype(str).str.strip().str.lower() #take out query params\n",
    "    s = s.str.replace(r'\\?.*$', '', regex=True) #leave with the last part after the last /\n",
    "    s = s.str.extract(r'/([^/]+)/?$')[0] #transform slug into normal title\n",
    "    s = (\n",
    "        s.str.replace('-', ' ', regex=False)\n",
    "         .str.strip()\n",
    "         .str.title()\n",
    "    ) #normalice title\n",
    "    s = s.replace({'': np.nan}) # replace empty for NaN\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d805b04",
   "metadata": {},
   "source": [
    "**Process Movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d41f3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metaClean43Brightspace.xlsx: 11364 rows, 13 cols\n",
      "\n",
      "Movies - URLs before and after title extraction:\n",
      "                                                 url  \\\n",
      "0     https://www.metacritic.com/movie/fantasia-2000   \n",
      "1  https://www.metacritic.com/movie/lupin-iii-the...   \n",
      "2       https://www.metacritic.com/movie/next-friday   \n",
      "3       https://www.metacritic.com/movie/my-dog-skip   \n",
      "4         https://www.metacritic.com/movie/supernova   \n",
      "5       https://www.metacritic.com/movie/down-to-you   \n",
      "6  https://www.metacritic.com/movie/things-you-ca...   \n",
      "7     https://www.metacritic.com/movie/the-big-tease   \n",
      "8           https://www.metacritic.com/movie/the-cup   \n",
      "9          https://www.metacritic.com/movie/santitos   \n",
      "\n",
      "                               title_from_url  \n",
      "0                               Fantasia 2000  \n",
      "1          Lupin Iii The Castle Of Cagliostro  \n",
      "2                                 Next Friday  \n",
      "3                                 My Dog Skip  \n",
      "4                                   Supernova  \n",
      "5                                 Down To You  \n",
      "6  Things You Can Tell Just By Looking At Her  \n",
      "7                               The Big Tease  \n",
      "8                                     The Cup  \n",
      "9                                    Santitos  \n",
      "Movies: rows,cols (11364, 14)\n",
      "Unique urls (non-null): 0\n",
      "Unique slugs: 0\n",
      "Example titles and ratings:\n",
      "                                        title rating\n",
      "0                               Fantasia 2000      G\n",
      "1         Lupin Iii: The Castle Of Cagliostro  PG-13\n",
      "2                                 Next Friday      R\n",
      "3                                 My Dog Skip     PG\n",
      "4                                   Supernova      R\n",
      "5                                 Down To You  PG-13\n",
      "6  Things You Can Tell Just By Looking At Her  PG-13\n",
      "7                               The Big Tease      R\n",
      "8                                     The Cup      G\n",
      "9                                    Santitos      R\n"
     ]
    }
   ],
   "source": [
    "base_path = Path(r\"C:\\Users\\dbust\\OneDrive\\Documentos\\Amsterdam_2025\\DDBM\\Database_Management\\Project_DBM\")\n",
    "\n",
    "df = load_excel(base_path, \"metaClean43Brightspace.xlsx\")  # load movies data\n",
    "\n",
    "if 'summary' in df.columns:\n",
    "    df_clean = df.drop(columns=['summary']).copy()  # drop summary column\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "# normalize columns\n",
    "df_clean = normalize_columns(df_clean, {\n",
    "    \"title\": [\"strip\", \"title\"],\n",
    "    \"studio\": [\"strip\", \"title\"],\n",
    "    \"rating\": [\"strip\", \"upper\"]   # will remove \"| \" separately\n",
    "})\n",
    "\n",
    "# remove leading \"| \" in rating\n",
    "if 'rating' in df_clean.columns:\n",
    "    df_clean['rating'] = df_clean['rating'].str.replace(r'^\\|\\s*', '', regex=True)\n",
    "\n",
    "# create movie_id sorted by release date if exists\n",
    "if 'RelDate' in df_clean.columns:\n",
    "    df_clean = df_clean.sort_values('RelDate').reset_index(drop=True)\n",
    "else:\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "df_clean['movie_id'] = range(1, len(df_clean) + 1)\n",
    "\n",
    "# convert cast and genre columns into lists\n",
    "if 'cast' in df_clean.columns:\n",
    "    df_clean['cast'] = df_clean['cast'].apply(split_and_strip)\n",
    "if 'genre' in df_clean.columns:\n",
    "    df_clean['genre'] = df_clean['genre'].apply(split_and_strip)\n",
    "\n",
    "# apply function to extract title from url\n",
    "if 'url' in df_clean.columns:\n",
    "    df_clean['title_from_url'] = extract_title_from_url(df_clean['url'])\n",
    "\n",
    "    # print before and after for the first 10\n",
    "    print(\"\\nMovies - URLs before and after title extraction:\")\n",
    "    print(df_clean[['url', 'title_from_url']].head(10))\n",
    "\n",
    "# prints for verification\n",
    "print(\"Movies: rows,cols\", df_clean.shape)\n",
    "print(\"Unique urls (non-null):\", df_clean['url_clean'].nunique(dropna=True) if 'url_clean' in df_clean else 0)\n",
    "print(\"Unique slugs:\", df_clean['slug'].nunique(dropna=True) if 'slug' in df_clean else 0)\n",
    "print(\"Example titles and ratings:\")\n",
    "print(df_clean[['title','rating']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c174378",
   "metadata": {},
   "source": [
    "**Process Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8507d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sales_movies.xlsx: 30612 rows, 16 cols\n",
      "\n",
      "Sales - URLs before and after title extraction:\n",
      "                                                 url  \\\n",
      "0  https://www.the-numbers.com/movie/Bakha-Satang...   \n",
      "1  https://www.the-numbers.com/movie/Looking-for-...   \n",
      "2      https://www.the-numbers.com/movie/Kurukshetra   \n",
      "3  https://www.the-numbers.com/movie/Little-Nicky...   \n",
      "4     https://www.the-numbers.com/movie/Suzhou-River   \n",
      "5  https://www.the-numbers.com/movie/Possible-Worlds   \n",
      "6  https://www.the-numbers.com/movie/Me-and-Isaac...   \n",
      "7    https://www.the-numbers.com/movie/Angels-Ladies   \n",
      "8  https://www.the-numbers.com/movie/Charlies-Angels   \n",
      "9  https://www.the-numbers.com/movie/Legend-of-Ba...   \n",
      "\n",
      "               title_from_url  \n",
      "0      Bakha Satang (S Korea)  \n",
      "1         Looking For An Echo  \n",
      "2                 Kurukshetra  \n",
      "3         Little Nicky (2000)  \n",
      "4                Suzhou River  \n",
      "5             Possible Worlds  \n",
      "6         Me And Isaac Newton  \n",
      "7               Angels Ladies  \n",
      "8             Charlies Angels  \n",
      "9  Legend Of Bagger Vance The  \n",
      "\n",
      "Sales - URLs before and after slug extraction:\n",
      "                                                 url  \\\n",
      "0  https://www.the-numbers.com/movie/Bakha-Satang...   \n",
      "1  https://www.the-numbers.com/movie/Looking-for-...   \n",
      "2      https://www.the-numbers.com/movie/Kurukshetra   \n",
      "3  https://www.the-numbers.com/movie/Little-Nicky...   \n",
      "4     https://www.the-numbers.com/movie/Suzhou-River   \n",
      "5  https://www.the-numbers.com/movie/Possible-Worlds   \n",
      "6  https://www.the-numbers.com/movie/Me-and-Isaac...   \n",
      "7    https://www.the-numbers.com/movie/Angels-Ladies   \n",
      "8  https://www.the-numbers.com/movie/Charlies-Angels   \n",
      "9  https://www.the-numbers.com/movie/Legend-of-Ba...   \n",
      "\n",
      "                         slug  \n",
      "0      Bakha Satang (S Korea)  \n",
      "1         Looking For An Echo  \n",
      "2                 Kurukshetra  \n",
      "3         Little Nicky (2000)  \n",
      "4                Suzhou River  \n",
      "5             Possible Worlds  \n",
      "6         Me And Isaac Newton  \n",
      "7               Angels Ladies  \n",
      "8             Charlies Angels  \n",
      "9  Legend Of Bagger Vance The  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['slug'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df_sales_clean[[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mslug\u001b[39m\u001b[33m'\u001b[39m]].head(\u001b[32m10\u001b[39m))\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# --- Merge sales with movies using slug ---\u001b[39;00m\n\u001b[32m     28\u001b[39m df_sales_merged = df_sales_clean.merge(\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mdf_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mslug\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,   \u001b[38;5;66;03m# keep only id + slug from movies\u001b[39;00m\n\u001b[32m     30\u001b[39m     on=\u001b[33m'\u001b[39m\u001b[33mslug\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     31\u001b[39m     how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# keep all sales, even if no match\u001b[39;00m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# --- Check 1: Non-matches (sales that didn't find a movie_id) ---\u001b[39;00m\n\u001b[32m     35\u001b[39m non_matches = df_sales_merged[df_sales_merged[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m].isna()]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dbust\\anaconda3\\envs\\DtaBaseProject\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dbust\\anaconda3\\envs\\DtaBaseProject\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dbust\\anaconda3\\envs\\DtaBaseProject\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['slug'] not in index\""
     ]
    }
   ],
   "source": [
    "df_sales = load_excel(base_path, \"sales_movies.xlsx\")  # load sales data\n",
    "\n",
    "# drop unnecessary columns if they exist\n",
    "to_drop = ['Unnamed: 8','opening_weekend', 'theatre_count','avg run per theatre', 'creative_type']\n",
    "df_sales_clean = df_sales.drop(columns=[c for c in to_drop if c in df_sales.columns]).copy()\n",
    "df_sales_clean.columns = df_sales_clean.columns.str.strip()  # clean column names\n",
    "\n",
    "# create sales_id sorted by year\n",
    "df_sales_clean = df_sales_clean.sort_values('year').reset_index(drop=True)\n",
    "df_sales_clean['sales_id'] = range(1, len(df_sales_clean) + 1)\n",
    "\n",
    "# apply function to extract title from url\n",
    "if 'url' in df_sales_clean.columns:\n",
    "    df_sales_clean['title_from_url'] = extract_title_from_url(df_sales_clean['url'])\n",
    "\n",
    "    # print before and after for the first 10\n",
    "    print(\"\\nSales - URLs before and after title extraction:\")\n",
    "    print(df_sales_clean[['url', 'title_from_url']].head(10))\n",
    "\n",
    "# apply function to extract slug from url (not only title)\n",
    "if 'url' in df_sales_clean.columns:\n",
    "    df_sales_clean['slug'] = extract_title_from_url(df_sales_clean['url'])\n",
    "    # print before and after for the first 10\n",
    "    print(\"\\nSales - URLs before and after slug extraction:\")\n",
    "    print(df_sales_clean[['url', 'slug']].head(10))\n",
    "\n",
    "# --- Merge sales with movies using slug ---\n",
    "df_sales_merged = df_sales_clean.merge(\n",
    "    df_clean[['movie_id', 'slug']],   # keep only id + slug from movies\n",
    "    on='slug',\n",
    "    how='left'  # keep all sales, even if no match\n",
    ")\n",
    "\n",
    "# --- Check 1: Non-matches (sales that didn't find a movie_id) ---\n",
    "non_matches = df_sales_merged[df_sales_merged['movie_id'].isna()]\n",
    "print(f\"\\nNumber of non-matching slugs: {len(non_matches)}\")\n",
    "if len(non_matches) > 0:\n",
    "    print(\"Non-matching slugs list:\")\n",
    "    print(non_matches['slug'].unique())\n",
    "\n",
    "# --- Check 2: Duplicates (slugs that matched multiple movie_ids) ---\n",
    "dup_sales = df_sales_merged[df_sales_merged.duplicated(subset=['slug'], keep=False)]\n",
    "print(f\"\\nNumber of duplicated slugs in merge: {dup_sales['slug'].nunique()}\")\n",
    "if len(dup_sales) > 0:\n",
    "    print(\"Duplicated slugs list:\")\n",
    "    print(dup_sales[['slug', 'movie_id']].drop_duplicates())\n",
    "\n",
    "# prints for verification\n",
    "#print(\"Sales: rows,cols\", df_sales_clean.shape)\n",
    "#print(\"Sales unique urls:\", df_sales_clean['url_clean'].nunique(dropna=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtaBaseProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
