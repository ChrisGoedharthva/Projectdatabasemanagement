{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039e2d9f",
   "metadata": {},
   "source": [
    "Fifth Draft- Tried again to match the movies id through the slugs of the urls, but this time generating the movie id from the merge dataframe with the slugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656bf81",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af5fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       # data manipulation\n",
    "import numpy as np        # numerical operations\n",
    "import re                 # regular expressions\n",
    "from pathlib import Path  # handle filesystem paths\n",
    "\n",
    "# function to load excel file and print rows and columns\n",
    "def load_excel(path: Path, filename: str):\n",
    "    file = path / filename                          # create full path\n",
    "    if not file.exists():                           # check if file exists\n",
    "        raise FileNotFoundError(f\"File not found: {file}\")\n",
    "    df = pd.read_excel(file, engine=\"openpyxl\")     # read excel file with openpyxl\n",
    "    print(f\"Loaded {filename}: {df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "    return df\n",
    "\n",
    "# function to normalize text columns with a list of steps\n",
    "def normalize_columns(df: pd.DataFrame, transformations: dict):\n",
    "    for col, steps in transformations.items():        # iterate over column and steps\n",
    "        if col not in df.columns:                     # skip if column not present\n",
    "            continue\n",
    "        s = df[col].astype(\"string\")                  # convert column to string dtype\n",
    "        for step in steps:                            # iterate transformations\n",
    "            if isinstance(step, str):\n",
    "                if step == \"strip\":\n",
    "                    s = s.str.strip()                # remove leading/trailing spaces\n",
    "                elif step == \"lower\":\n",
    "                    s = s.str.lower()                # convert to lowercase\n",
    "                elif step == \"upper\":\n",
    "                    s = s.str.upper()                # convert to uppercase\n",
    "                elif step == \"title\":\n",
    "                    s = s.str.title()                # title case\n",
    "                else:\n",
    "                    if hasattr(s.str, step):        # fallback for other string methods\n",
    "                        s = getattr(s.str, step)()\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown step '{step}' for column {col}\")\n",
    "            elif isinstance(step, (tuple, list)) and step[0] == \"replace\":\n",
    "                _, pat, repl, *rest = step\n",
    "                regex = rest[0] if rest else False\n",
    "                s = s.str.replace(pat, repl, regex=regex)   # replace pattern\n",
    "            elif callable(step):\n",
    "                try:\n",
    "                    res = step(s)                     # try vectorized\n",
    "                    if isinstance(res, (pd.Series, np.ndarray, list)):\n",
    "                        s = pd.Series(res, index=s.index)\n",
    "                    else:\n",
    "                        s = s.apply(step)\n",
    "                except Exception:\n",
    "                    s = s.apply(step)                 # fallback to apply\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported transformation step: \" + repr(step))\n",
    "        df[col] = s                                  # assign back to df\n",
    "    return df\n",
    "\n",
    "# split comma-separated strings into list and strip spaces\n",
    "def split_and_strip(s):\n",
    "    if pd.isna(s) or str(s).strip() == '':\n",
    "        return []\n",
    "    return [item.strip() for item in str(s).split(',') if item.strip() != '']\n",
    "\n",
    "# unified slug extraction from url for both movies and sales\n",
    "def make_slug(series: pd.Series) -> pd.Series:\n",
    "    s = series.fillna('').astype(str).str.strip().str.lower()\n",
    "    s = s.str.replace(r'\\?.*$', '', regex=True)       # remove query params\n",
    "    s = s.str.extract(r'/([^/]+)/?$')[0]             # take last part of url\n",
    "    s = (\n",
    "        s.str.replace('-', ' ', regex=False)         # replace dashes with spaces\n",
    "         .str.replace(r'\\(.*?\\)', '', regex=True)    # remove parentheses like (2000)\n",
    "         .str.strip()\n",
    "         .str.title()\n",
    "    )\n",
    "    s = s.replace({'': np.nan})\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478c446",
   "metadata": {},
   "source": [
    "**Process Movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d04a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metaClean43Brightspace.xlsx: 11364 rows, 13 cols\n",
      "\n",
      "Movies - URLs before and after slug extraction:\n",
      "                                                 url  \\\n",
      "0     https://www.metacritic.com/movie/fantasia-2000   \n",
      "1  https://www.metacritic.com/movie/lupin-iii-the...   \n",
      "2       https://www.metacritic.com/movie/next-friday   \n",
      "3       https://www.metacritic.com/movie/my-dog-skip   \n",
      "4         https://www.metacritic.com/movie/supernova   \n",
      "5       https://www.metacritic.com/movie/down-to-you   \n",
      "6  https://www.metacritic.com/movie/things-you-ca...   \n",
      "7     https://www.metacritic.com/movie/the-big-tease   \n",
      "8           https://www.metacritic.com/movie/the-cup   \n",
      "9          https://www.metacritic.com/movie/santitos   \n",
      "\n",
      "                                         slug  \n",
      "0                               Fantasia 2000  \n",
      "1          Lupin Iii The Castle Of Cagliostro  \n",
      "2                                 Next Friday  \n",
      "3                                 My Dog Skip  \n",
      "4                                   Supernova  \n",
      "5                                 Down To You  \n",
      "6  Things You Can Tell Just By Looking At Her  \n",
      "7                               The Big Tease  \n",
      "8                                     The Cup  \n",
      "9                                    Santitos  \n",
      "Movies: rows,cols (11364, 13)\n",
      "Unique slugs: 11364\n"
     ]
    }
   ],
   "source": [
    "base_path = Path(r\"C:\\Users\\dbust\\OneDrive\\Documentos\\Amsterdam_2025\\DDBM\\Database_Management\\Project_DBM\")\n",
    "\n",
    "df = load_excel(base_path, \"metaClean43Brightspace.xlsx\")  # load movies data\n",
    "\n",
    "if 'summary' in df.columns:\n",
    "    df_clean = df.drop(columns=['summary']).copy()  # drop summary column\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "# normalize columns\n",
    "df_clean = normalize_columns(df_clean, {\n",
    "    \"title\": [\"strip\", \"title\"],\n",
    "    \"studio\": [\"strip\", \"title\"],\n",
    "    \"rating\": [\"strip\", \"upper\"]   # will remove \"| \" separately\n",
    "})\n",
    "\n",
    "# remove leading \"| \" in rating\n",
    "if 'rating' in df_clean.columns:\n",
    "    df_clean['rating'] = df_clean['rating'].str.replace(r'^\\|\\s*', '', regex=True)\n",
    "\n",
    "# sort by release date if exists\n",
    "if 'RelDate' in df_clean.columns:\n",
    "    df_clean = df_clean.sort_values('RelDate').reset_index(drop=True)\n",
    "else:\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "# apply unified slug function from URLs\n",
    "if 'url' in df_clean.columns:\n",
    "    df_clean['slug'] = make_slug(df_clean['url'])  # create slug from movie URLs\n",
    "    print(\"\\nMovies - URLs before and after slug extraction:\")\n",
    "    print(df_clean[['url', 'slug']].head(10))\n",
    "\n",
    "# convert cast and genre columns into lists\n",
    "if 'cast' in df_clean.columns:\n",
    "    df_clean['cast'] = df_clean['cast'].apply(split_and_strip)\n",
    "if 'genre' in df_clean.columns:\n",
    "    df_clean['genre'] = df_clean['genre'].apply(split_and_strip)\n",
    "\n",
    "# prints for verification\n",
    "print(\"Movies: rows,cols\", df_clean.shape)\n",
    "print(\"Unique slugs:\", df_clean['slug'].nunique(dropna=True) if 'slug' in df_clean else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec383ad",
   "metadata": {},
   "source": [
    "**Process Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sales_movies.xlsx: 30612 rows, 16 cols\n",
      "Sales: rows,cols (30612, 13)\n",
      "Unique slugs: 29944\n"
     ]
    }
   ],
   "source": [
    "df_sales = load_excel(base_path, \"sales_movies.xlsx\")  # load sales data\n",
    "\n",
    "# drop unnecessary columns if they exist\n",
    "to_drop = ['Unnamed: 8','opening_weekend', 'theatre_count','avg run per theatre', 'creative_type']\n",
    "df_sales_clean = df_sales.drop(columns=[c for c in to_drop if c in df_sales.columns]).copy()\n",
    "df_sales_clean.columns = df_sales_clean.columns.str.strip()  # clean column names\n",
    "\n",
    "# sort by year\n",
    "df_sales_clean = df_sales_clean.sort_values('year').reset_index(drop=True)\n",
    "df_sales_clean['sales_id'] = range(1, len(df_sales_clean) + 1)\n",
    "\n",
    "# apply unified slug function from URLs\n",
    "if 'url' in df_sales_clean.columns:\n",
    "    df_sales_clean['slug'] = make_slug(df_sales_clean['url'])  # create slug from sales URLs\n",
    "    #print(\"\\nSales - URLs before and after slug extraction:\")\n",
    "    #print(df_sales_clean[['url', 'slug']].head(10))\n",
    "\n",
    "# prints for verification\n",
    "print(\"Sales: rows,cols\", df_sales_clean.shape)\n",
    "print(\"Unique slugs:\", df_sales_clean['slug'].nunique(dropna=True) if 'slug' in df_sales_clean else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa237f7",
   "metadata": {},
   "source": [
    "**Merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5f6eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of non-matching slugs: 1\n",
      "First 10 non-matching slugs:\n",
      "[nan]\n",
      "\n",
      "Number of duplicated slugs in merge: 0\n",
      "\n",
      "Sales table with slugs and movie_id:\n",
      "                            slug  movie_id\n",
      "0                   Bakha Satang         1\n",
      "1            Looking For An Echo         2\n",
      "2                    Kurukshetra         3\n",
      "3                   Little Nicky         4\n",
      "4                   Suzhou River         5\n",
      "5                Possible Worlds         6\n",
      "6            Me And Isaac Newton         7\n",
      "7                  Angels Ladies         8\n",
      "8                Charlies Angels         9\n",
      "9     Legend Of Bagger Vance The        10\n",
      "10                      Restless        11\n",
      "11                     Blue Moon        12\n",
      "12  2 Manner 2 Frauen 4 Probleme        13\n",
      "13              Boesman And Lena        14\n",
      "14        Venus Beauty Institute        15\n",
      "15                 Mercy Streets        16\n",
      "16                        Kippur        17\n",
      "17                      Kikujiro        18\n",
      "18                 Shanghai Noon        19\n",
      "19               Passion Of Mind        20\n"
     ]
    }
   ],
   "source": [
    "# --- Merge sales with movies using slug only ---\n",
    "df_merged = df_sales_clean.merge(\n",
    "    df_clean[['slug']],   # only bring slug from movies\n",
    "    on='slug',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Assign movie_id based on unique slugs in merged dataset ---\n",
    "df_merged = df_merged.drop_duplicates(subset=['slug']).reset_index(drop=True)\n",
    "df_merged['movie_id'] = range(1, len(df_merged) + 1)\n",
    "\n",
    "# --- Check 1: Non-matches (sales slugs not found in movies) ---\n",
    "non_matches = df_merged[df_merged['slug'].isna()]\n",
    "print(f\"\\nNumber of non-matching slugs: {len(non_matches)}\")\n",
    "if len(non_matches) > 0:\n",
    "    print(\"First 10 non-matching slugs:\")\n",
    "    print(non_matches['slug'].head(10).tolist())\n",
    "\n",
    "# --- Check 2: Duplicates (slugs appearing multiple times) ---\n",
    "dup_slugs = df_merged[df_merged.duplicated(subset=['slug'], keep=False)]\n",
    "print(f\"\\nNumber of duplicated slugs in merge: {dup_slugs['slug'].nunique()}\")\n",
    "\n",
    "# --- Assign back movie_id to sales table ---\n",
    "df_sales_clean = df_sales_clean.merge(\n",
    "    df_merged[['slug', 'movie_id']],\n",
    "    on='slug',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nSales table with slugs and movie_id:\")\n",
    "print(df_sales_clean[['slug', 'movie_id']].head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtaBaseProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
